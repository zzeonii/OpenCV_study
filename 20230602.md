# 과일 이미지에 얼굴 합성하기
- dlib : C++로 작성된 범용 크로스 플랫폼 소프트웨어 라이브러리
- 얼굴을 우선적으로 잡고 대략적인 위치를 정하고 대략적인 위치의 좌표를 정해준다
  - 관련 파일 다운로드 : git clone -b test --single-branch https://github.com/AntonSangho/annoying-orange-face.git
  - https://github.com/davisking/dlib-models/blob/master/shape_predictor_68_face_landmarks.dat.bz2 파일 받아오기
  - 사용하고자하는 annoying-orange-face에 bz2파일 이동
  - bunzip2 shape_predictor_68_face_landmarks.dat.bz2 로 압축풀기
- 과일 이미지 크기조절
- 데이터 셋 가져오기
- 카메라를 열고 인식해주기
- 얼굴 인식을 해주고 눈, 입을 인식
- 인식 데이터를 crop으로 오려주고, 합성을 한다(cv2.seamlessClone)
- imutils가 없는 경우 pip3 install imutils로 설치
```python
import cv2
import numpy as np
import dlib
from imutils import face_utils, resize

# 과일 이미지 크기 조절
orange_img = cv2.imread('orange.jpg')
orange_img = cv2.resize(orange_img, dsize=(512, 512))
# 데이터 셋을 가져오기
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')
# 카메라 열기
# cap = cv2.VideoCapture(0)
# 동영상으로 하기
cap = cv2.VideoCapture('01.mp4')

while cap.isOpened():
    ret, img = cap.read()
    if not ret:
        break
    # 얼굴 인식
    faces = detector(img)
    result = orange_img.copy()
    # 얼굴이 한개 이상일 때
    if len(faces) > 0:
        face = faces[0]
        # 얼굴의 왼쪽, 위쪽, 오른쪽, 아래쪽의 좌표 저장하기
        x1, y1, x2, y2 = face.left(),face.top(), face.right(),face.bottom()
        # 얼굴만 복사하기
        face_img = img[y1:y2, x1:x2].copy()

        shape = predictor(img, face)
        shape = face_utils.shape_to_np(shape)

        for p in shape:
            cv2.circle(face_img, center=(p[0]-x1, p[1]-y1), radius=2, color=255, thickness=-1)

        # cv2.imshow('face', face_img)
    # if cv2.waitKey(1) == ord('q'):
    #     break

        # 왼쪽 눈
        # x축 36~39 y축 37~41
        le_x1 = shape[36, 0]
        le_y1 = shape[37, 1]
        le_x2 = shape[39, 0]
        le_y2 = shape[41, 1]
        # 마진을 주기
        le_margin = int((le_x2 - le_x1) * 0.18)
        
        # 오른쪽 눈
        # x축 42~45 y축 43~47
        re_x1 = shape[42, 0]
        re_y1 = shape[43, 1]
        re_x2 = shape[45, 0]
        re_y2 = shape[47, 1]
        # 마진을 주기
        re_margin = int((re_x2 - re_x1) * 0.18)
        
        # crop
        left_eye_img = img[le_y1-le_margin:le_y2+le_margin, le_x1-le_margin:le_x2+le_margin].copy()
        right_eye_img = img[re_y1-re_margin:re_y2+re_margin, re_x1-re_margin:re_x2+re_margin].copy()

        left_eye_img = resize(left_eye_img, width=100)
        right_eye_img = resize(right_eye_img, width=100)
        # 합성(poison blending)을 티 안나게 한다 # 합성 cv2.seamlessClone
        result = cv2.seamlessClone(
            left_eye_img,
            result,
            np.full(left_eye_img.shape[:2], 255, left_eye_img.dtype),
            (100,200),
            # 알아서 해
            cv2.MIXED_CLONE
        )
        result = cv2.seamlessClone(
            right_eye_img,
            result,
            np.full(right_eye_img.shape[:2], 255, right_eye_img.dtype),
            (250,200),
            # 알아서 해
            cv2.MIXED_CLONE
        )

    #     cv2.imshow('left', left_eye_img)
    #     cv2.imshow('right', right_eye_img)
    # if cv2.waitKey(1) == ord('q'):
    #     break

        # 입
        mouth_x1 = shape[48, 0]
        mouth_y1 = shape[50, 1]
        mouth_x2 = shape[54, 0]
        mouth_y2 = shape[57, 1]
        mouth_margin = int((mouth_x2 - mouth_x1)*0.1)

        mouth_img = img[mouth_y1-mouth_margin:mouth_y2+mouth_margin, mouth_x1-mouth_margin:mouth_x2+mouth_margin].copy()

        mouth_img = resize(mouth_img, width=250)

        result = cv2.seamlessClone(
            mouth_img,
            result,
            np.full(mouth_img.shape[:2],255, mouth_img.dtype),
            (180, 320),
            cv2.MIXED_CLONE
        )
    # 입 실행
    #     cv2.imshow('mouth', mouth_img)
    # if cv2.waitKey(1) == ord('q'):
    #     break
    
    # 결과
        cv2.imshow('result', result)
    if cv2.waitKey(1) == ord('q'):
        break
```
![image](https://github.com/zzeonii/OpenCV_study/assets/129237950/f5a8bc5a-09e8-4a4d-9de7-ad47cf1f84d9)

# 사람 얼굴에 라이언 스티커 합성
```python
import cv2, dlib, sys
import numpy as np

scaler = 0.3

# initialize face detector and shape predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')

# 1. load video
cap = cv2.VideoCapture('samples/girl.mp4')
# load overlay image
overlay = cv2.imread('samples/ryan_transparent.png', cv2.IMREAD_UNCHANGED)

# overlay function
def overlay_transparent(background_img, img_to_overlay_t, x, y, overlay_size=None):
  try:
    bg_img = background_img.copy()
    # convert 3 channels to 4 channels
    if bg_img.shape[2] == 3:
      bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGR2BGRA)

    if overlay_size is not None:
      img_to_overlay_t = cv2.resize(img_to_overlay_t.copy(), overlay_size)

    b, g, r, a = cv2.split(img_to_overlay_t)

    mask = cv2.medianBlur(a, 5)

    h, w, _ = img_to_overlay_t.shape
    roi = bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)]

    img1_bg = cv2.bitwise_and(roi.copy(), roi.copy(), mask=cv2.bitwise_not(mask))
    img2_fg = cv2.bitwise_and(img_to_overlay_t, img_to_overlay_t, mask=mask)

    bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)] = cv2.add(img1_bg, img2_fg)

    # convert 4 channels to 4 channels
    bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGRA2BGR)

    return bg_img
  except Exception:return background_img

face_roi = []
face_sizes = []

# loop
# 2. 비디오를 계속 프레임 단위로 읽어야하므로
while True:
  # read frame buffer from video
  ret, img = cap.read()
  # 프레임이 없으면 종료 
  if not ret:
    break

  # resize frame
  img = cv2.resize(img, (int(img.shape[1] * scaler), int(img.shape[0] * scaler)))
  ori = img.copy()

  # find faces
  if len(face_roi) == 0:
    faces = detector(img, 1)
  else:
    roi_img = img[face_roi[0]:face_roi[1], face_roi[2]:face_roi[3]]
    # cv2.imshow('roi', roi_img)
    faces = detector(roi_img)

  # no faces
  if len(faces) == 0:
    print('no faces!')

  # find facial landmarks
  for face in faces:
    if len(face_roi) == 0:
      dlib_shape = predictor(img, face)
      shape_2d = np.array([[p.x, p.y] for p in dlib_shape.parts()])
    else:
      dlib_shape = predictor(roi_img, face)
      shape_2d = np.array([[p.x + face_roi[2], p.y + face_roi[0]] for p in dlib_shape.parts()])

    for s in shape_2d:
      cv2.circle(img, center=tuple(s), radius=1, color=(255, 255, 255), thickness=2, lineType=cv2.LINE_AA)

    # compute face center
    #center_x, center_y = np.mean(shape_2d, axis=0).astype(np.int)
    center_x, center_y = np.mean(shape_2d, axis=0).astype(np.int32)

    # compute face boundaries
    min_coords = np.min(shape_2d, axis=0)
    max_coords = np.max(shape_2d, axis=0)

    # draw min, max coords
    cv2.circle(img, center=tuple(min_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)
    cv2.circle(img, center=tuple(max_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)

    # compute face size
    face_size = max(max_coords - min_coords)
    face_sizes.append(face_size)
    if len(face_sizes) > 10:
      del face_sizes[0]
    mean_face_size = int(np.mean(face_sizes) * 1.8)

    # compute face roi
    face_roi = np.array([int(min_coords[1] - face_size / 2), int(max_coords[1] + face_size / 2), int(min_coords[0] - face_size / 2), int(max_coords[0] + face_size / 2)])
    face_roi = np.clip(face_roi, 0, 10000)

    # draw overlay on face
    result = overlay_transparent(ori, overlay, center_x + 8, center_y - 25, overlay_size=(mean_face_size, mean_face_size))

  # visualize
  cv2.imshow('original', ori)
  cv2.imshow('facial landmarks', img)
  cv2.imshow('result', result)
  if cv2.waitKey(1) == ord('q'):
    sys.exit(1)
```

![image](https://github.com/zzeonii/OpenCV_study/assets/129237950/1e10c5e8-0d6a-4079-adef-e3d14f934af2)


